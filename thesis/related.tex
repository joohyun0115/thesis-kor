\section{Related work} \label{sec:RelatedWork}
%$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$
%Paragraph 1:Linux Scalability의 연구에 대한 설명
%$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$

\noindent
\textbf{Operating system scalability.}
~\cite{Clements15SCR}In order to improve Linux scalability, researchers have
been optimized memory management in Linux by finding and fixing scalability
bottlenecks~\cite{BoydWickizer2008Corey}~\cite{BoydWickizer2012OLS}.
Shared address spaces in multithreaded applications 
easily become scalability bottlenecks since kernel operations 
including \code{mmap} and \code{munmap} system calls and \code{page faults}
 handling require per-process locks for synchronization.
Multithreaded application, for example, can become bottleneck by kernel
operations on their shared address space, whose operations are the \code{mmap}
and \code{munmap} system calls and \code{page faults}.
These operations are synchronized by a single per-process lock.
BonsaiVM~\cite{AustinTClements2012RCUBalancedTrees} solved this address space
problem by using the RCU;
RadixVM~\cite{Clements2013RadixVM} created a new VM using refcache and radix
tree, which enable \code{munmap}, \code{mmap}, and \code{page fault} on
non-overlapping memory regions to scale perfectly.
Alternatively, to avoid contention caused by shared address space locking,
system programmers change their multithreaded applications to use
processes~\cite{SilasBoydWickizer2010LinuxScales48}.


%$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$
%Paragraph 2:Concurrent updates에 대한 연구
%$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$
\noindent
\textbf{Scalabe data structure.}
~\cite{Dodds2015SCT}Though sufficient level of performance scalability has been
achieved for reader intensive operations through RCU and Hazard pointer, 
solutions to scalability for update-heavy operations has not been satisfiable.
A recent paper by Arbel and Attiya~\cite{Arbel2014ConcurrentRCU} shows a new
design of concurrent search tree called the Citrus tree. The Citrus tree
combines RCU and fine-grained locks, and it supports concurrent write
operations that traverse the search tree by using RCU concurrently.
When increasing the update rate, Citrus tree still suffers from bottlenecks.
RLU~\cite{Matveev2015RLU} presents a new synchronization mechanism that allows
unsynchronized sequences of reads to execute concurrently with updates.
In high update rate, Oplog can achieve substantially multi-core scaling for
update-heavy data structures.
Our work focus on update-heavy data structures and uses non-blocking method to
 store the operation log instead of per-core processing.


%$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$
%Paragraph 3: Scalable Data Structure and Lock에 대한 연구
%$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$
\noindent
\textbf{Scalable lock.}
~\cite{Wang2016BeMyGuest}~\cite{Bueso2015STP}~\cite{Bueso2014MCS}One method for
the concurrent update is using the non-blocking algorithms~\cite{Harris2001Lockfree}~\cite{Fomitchev2004Lockfree}~\cite{Timnat2012},
 which are based on CAS.


MCS~\cite{MellorCrummey91}, a scalable exclusive lock, is used in the Linux
kernel~\cite{MCSLocksKernel}.
to avoid unfairness at high contention levels, so this scalable exclusive
lock can be used for fine grained locking in Linux. 
However, in MCS, since only one thread may hold the lock at a time, it can
cause low scalability in case of long critical regions.
Reader-writer lock~\cite{Courtois71} allows either number of readers to execute
concurrently or single writer to execute.
Thus, readers-writer locks allow better scalability in case of read-mostly 
objects.

In read-mostly data structures, RCU~\cite{McKenney98} can be quite useful
since it allows read operations to proceed without read locks, and delays
freeing of data structures to avoid races. One drawback is that as the update
rate increases, their performance and scalability decrease due to a single
writer and their synchronization function.
Consequently, 
scalable exclusive lock, reader-writer lock
and RCU require serialization for updates and thus show significant limitation
 on scalability. 



%$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$
%$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$
%Reference Sentence 1
%$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$




%$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$
%Reference Sentence 2:LDU paper
%$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$

%Paragraph 1:Linux Scalability의 연구에 대한 설명
%In order to improve Linux scalability, researchers have been optimized memory
%management in Linux by finding and fixing scalability bottlenecks.

%Shared address spaces in multithreaded applications 
%easily become scalability bottlenecks since kernel operations 
%including \code{mmap} and \code{munmap} system calls and \code{page faults}
% handling require per-process locks for synchronization.
%Multithreaded application, for example, can become bottleneck by kernel
%operations on their shared address space, whose operations are the \code{mmap}
%and \code{munmap} system calls and \code{page faults}.
%These operations are synchronized by a single per-process lock.
%BonsaiVM~\cite{AustinTClements2012RCUBalancedTrees} solved this address space
%problem by using the RCU;
%RadixVM~\cite{Clements2013RadixVM} created a new VM using refcache and radix
%tree, which enable \code{munmap}, \code{mmap}, and \code{page fault} on
%non-overlapping memory regions to scale perfectly.
%Alternatively, to avoid contention caused by shared address space locking,
%system programmers change their multithreaded applications to use
%processes~\cite{SilasBoydWickizer2010LinuxScales48}.
 
%Paragraph 2:Fork Scalability에 대한 설명
%Multi-processing environment 
%suffers from scalability bottlenecks due to the well-known fork
%scalability problem~\cite{Andi2011adding}~\cite{Tim2013adding}.
%When Linux spawns child process, the Linux substantially performs locks because
%of protecting the reverse mapping data structure naturally causing
%bottlenecks.
%Oplog~\cite{SilasBoydWickizerPth}, which is an important basis of our approach,
%solves this problem using time-stamp log and
%per-core processing.


%\subsection{Locking}
%Paragraph 1: Locks에 대한 설명
%MCS~\cite{MellorCrummey91}, a scalable exclusive lock, is used in the Linux
%kernel~\cite{MCSLocksKernel}.
%to avoid unfairness at high contention levels, so this scalable exclusive
%lock can be used for fine grained locking in Linux. 
%However, in MCS, since only one thread may hold the lock at a time, it can
%cause low scalability in case of long critical regions.
%Reader-writer lock~\cite{Courtois71} allows either number of readers to execute
%concurrently or single writer to execute.
%Thus, readers-writer locks allow better scalability in case of read-mostly 
%objects.

%In read-mostly data structures, RCU~\cite{McKenney98} can be quite useful
%since it allows read operations to proceed without read locks, and delays
%freeing of data structures to avoid races. One drawback is that as the update
%rate increases, their performance and scalability decrease due to a single
%writer and their synchronization function.
%Consequently, 
%scalable exclusive lock, reader-writer lock
%and RCU require serialization for updates and thus show significant limitation
% on scalability. 

%\subsection{Non-Blocking algorithm}

%Paragraph 1: Lock-free 방법 설명
%One method for the concurrent update is using the non-blocking
%algorithms~\cite{Harris2001Lockfree}~\cite{Fomitchev2004Lockfree}~\cite{Timnat2012},
% which are based on CAS.
%In non-blocking algorithms, each core tries to read the values of shared
%data structures from its local location, but has possibility of reading 
%obsolete values.
%CAS is performed at the time of reading values that are not the current values
%and CAS fails and requires retrials sometimes when the values have been
% overwritten.
%These algorithms execute optimistically as though they read the value at
%location in their data structure;they may obtain stale data at the time.
%When they observed against the current value, they execute a CAS to compare the
%against value.
%The CAS fails when the value has been overridden, and they must be
%retried later on.
%Consequently, both repeated CAS operation and their iteration loop caused by
%CAS fails cause bottlenecks due to inter-core communication
% overheads~\cite{SilasBoydWickizerPth}.
%Moreover, none of the non-blocking algorithms implements an iterator, whose
%data structure just consists of the insert, delete and contains
%operations~\cite{petrank2013lock}.
%The Linux, however, commonly uses the iteration to read, so when applying
%non-blocking algorithms to the Linux, they may meet this iteration problem.
%Petrank~\cite{petrank2013lock} solved this problem by using a consistent
%snapshot of the data structure; this method, however, may require a lot of
%effort to apply its sophisticated algorithms to Linux.
%For evaluation purposes, we implemented Harris linked
%list~\cite{Harris2001Lockfree} to Linux, and we sometimes have failure where
%reading the pointer that had been deleted by updater concurrently result of the
%problem of the iteration.

%Paragraph 2: Linux llist 설명
%Linux kernel uses lock-less list("lock-less NULL terminated single
%list") that are widely used in the Linux kernel to improve scalability.
%In order to delete operation for multiple consumer, the existing
%algorithms traverse the list from beginning or their optimized point.
%On the other hand, lock-less list inserts node at the first of the list, so
% when the CAS operation fails, they will minimally traverse from the head
% node.
%Although lock-less list uses a non blocking method, they retries minimally
%and thus they can significantly reduce inter-core communication bottleneck and
% the repeated loop bottleneck.
%Our proposed method uses this feature in case of inserting the operation log.

%\subsection{Concurrent Update}
%Paragraph 1: Update Rate
%Though sufficient level of performance scalability has been achieved for 
%reader intensive operations through RCU and Hazard pointer, 
%solutions to scalability for update-heavy operations has not been satisfiable.
%A recent paper by Arbel and Attiya~\cite{Arbel2014ConcurrentRCU} shows a new
%design of concurrent search tree called the Citrus tree. The Citrus tree
%combines RCU and fine-grained locks, and it supports concurrent write
%operations that traverse the search tree by using RCU concurrently.
%When increasing the update rate, Citrus tree still suffers from bottlenecks.
%RLU~\cite{Matveev2015RLU} presents a new synchronization mechanism that allows
%unsynchronized sequences of reads to execute concurrently with updates.
%In high update rate, Oplog can achieve substantially multi-core scaling for
%update-heavy data structures.
%Our work focus on update-heavy data structures and uses non-blocking method to
% store the operation log instead of per-core processing.
